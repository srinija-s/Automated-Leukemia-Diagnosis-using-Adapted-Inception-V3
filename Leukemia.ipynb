{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOiFbFKhQqZF",
        "outputId": "827fc05c-6d71-433b-a388-d7d031e3795f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTD3wO2DdNbN"
      },
      "source": [
        "# Data splitting\n",
        "70:30\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the source directory containing the folders\n",
        "source_dir = 'Images'\n",
        "\n",
        "# Define the destination directories for train and test sets\n",
        "train_dir = 'Train'\n",
        "test_dir = 'Test'\n",
        "\n",
        "# Ensure the destination folders exist; create them if they don't\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# List of folders inside the source directory\n",
        "folders = ['ALL', 'AML', 'CML', 'CLL', 'Healthy']\n",
        "\n",
        "# Split each folder into train and test sets\n",
        "for folder in folders:\n",
        "    # Get the list of files in the current folder\n",
        "    files = os.listdir(os.path.join(source_dir, folder))\n",
        "    # Shuffle the list of files randomly\n",
        "    random.shuffle(files)\n",
        "    # Calculate the number of files for the training set (70%) and test set (30%)\n",
        "    num_train = int(0.7 * len(files))\n",
        "    num_test = len(files) - num_train\n",
        "    # Move files to the train set\n",
        "    for filename in files[:num_train]:\n",
        "        source_file = os.path.join(source_dir, folder, filename)\n",
        "        dest_file = os.path.join(train_dir, folder, filename)\n",
        "        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
        "        shutil.move(source_file, dest_file)\n",
        "    # Move files to the test set\n",
        "    for filename in files[num_train:]:\n",
        "        source_file = os.path.join(source_dir, folder, filename)\n",
        "        dest_file = os.path.join(test_dir, folder, filename)\n",
        "        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
        "        shutil.move(source_file, dest_file)\n",
        "\n",
        "print(\"Files split into train and test sets successfully!\")"
      ],
      "metadata": {
        "id": "YRraaiL0EHoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYvVnkP0RJDH"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "(no of image, height, width, 3 = RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXC1jQiNXK5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93c7f7c-55fd-40c4-f266-02cd80c02430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (2463, 299, 299, 3)\n",
            "Shape of y_train: (2463,)\n",
            "Shape of X_test: (1059, 299, 299, 3)\n",
            "Shape of y_test: (1059,)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2, os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def preprocess_images(images_dir, image_size=(299, 299), augment=False):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "\n",
        "    # Get the list of class directories\n",
        "    class_dirs = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
        "\n",
        "    # Loop through each class directory\n",
        "    for class_dir in class_dirs:\n",
        "        class_label = class_dir\n",
        "\n",
        "        # Get the list of image files in the class directory\n",
        "        image_files = [f for f in os.listdir(os.path.join(images_dir, class_dir)) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "        # Loop through each image file\n",
        "        for filename in image_files:\n",
        "            # Read the image\n",
        "            img = cv2.imread(os.path.join(images_dir, class_dir, filename))\n",
        "            # Resize the image\n",
        "            img = cv2.resize(img, image_size)\n",
        "            # Convert the image to float32 and normalize pixel values to the range [0, 1]\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "            # Convert the image to array\n",
        "            img_array = img_to_array(img)\n",
        "            # Append the image array to X\n",
        "            X.append(img_array)\n",
        "            # Append the class label to y\n",
        "            y.append(class_label)\n",
        "\n",
        "    # Encode the class labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Convert X and y to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Define directories containing your images\n",
        "train_dir = '/content/drive/MyDrive/Multi/Train'\n",
        "test_dir = '/content/drive/MyDrive/Multi/Test'\n",
        "\n",
        "# Preprocess the resized images in the train directory with data augmentation\n",
        "X_train, y_train = preprocess_images(train_dir)\n",
        "\n",
        "# Preprocess the resized images in the test directory without data augmentation\n",
        "X_test, y_test = preprocess_images(test_dir)\n",
        "\n",
        "# Print the shapes of the preprocessed data\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW4YIVcYck72"
      },
      "source": [
        "# Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3FMuXDFaZLp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Load the InceptionV3 base model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "\n",
        "# Add custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)  # Adding dropout for regularization\n",
        "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)  # Add L2 regularization\n",
        "predictions = Dense(5, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model except the last few\n",
        "for layer in base_model.layers[:-5]:  # Fine-tuning by unfreezing last few layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag4hpeMVZWMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09b12fb-944b-4985-dbbb-80bf328190b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2463 images belonging to 5 classes.\n",
            "Found 1059 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define directories containing preprocessed images\n",
        "train_dir = '/content/drive/MyDrive/Multi/Train'\n",
        "test_dir = '/content/drive/MyDrive/Multi/Test'\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerator for training and testing data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches using ImageDataGenerator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Flow test images in batches using ImageDataGenerator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE3B9EfKcq10"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZuIGW1ea04X"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5, #number of epochs \n",
        "    validation_data=test_generator\n",
        ")"
      ]
    },
    {
  "cell_type": "code",
  "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_pred contains the predicted labels for the test data\n",
    "# Replace y_pred with your actual predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
  ],
  "metadata": {
    "id": "gy6GttF7KJD5"
  },
  "execution_count": null,
  "outputs": []
},
{
  "cell_type": "code",
  "source": [
    "# Assuming your trained model object is named 'model' and you want to save it to a file named 'my_model.h5'\n",
    "model.save('my_model.h5')\n"
  ],
  "metadata": {
    "id": "qIKsSnfofBtl"
  },
  "execution_count": null,
  "outputs": []
},
{
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
    "id": "ALkOLrYMw7WE"
  },
  "outputs": [],
  "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)\n"
  ]
},
{
  "cell_type": "code",
  "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss and accuracy values from history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Testing loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Testing accuracy')\n",
    "plt.title('Training and Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"

  ],
  "metadata": {
    "id": "ePkvXz3BUrjh"
  },
  "execution_count": null,
  "outputs": []
}
],


  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
